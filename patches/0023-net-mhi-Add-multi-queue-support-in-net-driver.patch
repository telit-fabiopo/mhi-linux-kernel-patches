From 75a5f33283639e5588b63c0e3e11c052cf3ae184 Mon Sep 17 00:00:00 2001
From: Vivek Pernamitta <quic_vpernami@quicinc.com>
Date: Tue, 30 Apr 2024 15:59:00 +0530
Subject: [PATCH 23/26] net: mhi: Add multi queue support in net driver

Support multiple network queues in single MHI net dev
interface. Each TX/RX queue pair will be mapped to
multiple channels.

example:
Network  queue-0  needs to be bind to channel 100/101
and queue-1 needs to bind to channel 108/109.

Adding net_dev fops for queue_mapping where
skb_get_queue_mapping() we will get the MHI TX queue index
from  skb->queue_mapping. when data domes from net stack (xmit
net dev fops) it is then passed to respective mhi channel
based on queue_mapping index defined in the SKB.

When SKB is received from device it will map the corresponding
MHI channel data (i.e., index mapped in mhi_result.chan_idx)
to net_dev RX queue via Skb_record_queue_mapping().

Ex: When data is transmitted on TX0 it needs to be transmitted
for channel 100, similarly when data received on channel
101 that needs to be submitted to MHI queue RX0.

Change-Id: I4c0fc7860d072322f1dd7efc5239b84efbf71e83
Signed-off-by: Vivek Pernamitta <quic_vpernami@quicinc.com>
---
 drivers/net/mhi_net.c | 187 +++++++++++++++++++++++++++++++-----------
 1 file changed, 138 insertions(+), 49 deletions(-)

diff --git a/drivers/net/mhi_net.c b/drivers/net/mhi_net.c
index 0b9d37979133..062e4abbd623 100644
--- a/drivers/net/mhi_net.c
+++ b/drivers/net/mhi_net.c
@@ -31,13 +31,14 @@ struct mhi_net_stats {
 struct mhi_net_dev {
 	struct mhi_device *mdev;
 	struct net_device *ndev;
-	struct sk_buff *skbagg_head;
-	struct sk_buff *skbagg_tail;
+	struct sk_buff **skbagg_head;
+	struct sk_buff **skbagg_tail;
 	struct delayed_work rx_refill;
 	struct mhi_net_stats stats;
-	u32 rx_queue_sz;
+	u32 *rx_queue_sz;
 	int msg_enable;
 	unsigned int mru;
+	bool queue;
 };
 
 struct mhi_device_info {
@@ -54,7 +55,7 @@ static int mhi_ndo_open(struct net_device *ndev)
 	/* Carrier is established via out-of-band channel (e.g. qmi) */
 	netif_carrier_on(ndev);
 
-	netif_start_queue(ndev);
+	netif_tx_start_all_queues(ndev);
 
 	return 0;
 }
@@ -63,7 +64,7 @@ static int mhi_ndo_stop(struct net_device *ndev)
 {
 	struct mhi_net_dev *mhi_netdev = netdev_priv(ndev);
 
-	netif_stop_queue(ndev);
+	netif_tx_stop_all_queues(ndev);
 	netif_carrier_off(ndev);
 	cancel_delayed_work_sync(&mhi_netdev->rx_refill);
 
@@ -75,8 +76,19 @@ static netdev_tx_t mhi_ndo_xmit(struct sk_buff *skb, struct net_device *ndev)
 	struct mhi_net_dev *mhi_netdev = netdev_priv(ndev);
 	struct mhi_device *mdev = mhi_netdev->mdev;
 	int err;
+	unsigned int queue = 0;
+	struct netdev_queue *txq;
 
 	err = mhi_queue_skb(mdev, DMA_TO_DEVICE, skb, skb->len, MHI_EOT);
+	/* Get the queue mapping number to transfer to the right channel*/
+	if (mhi_netdev->queue)
+		queue = skb_get_queue_mapping(skb);
+
+	if (queue > mdev->num_tx_chan)
+		return -ENODEV;
+
+	err = mhi_queue_skb_ch(mdev, DMA_TO_DEVICE, skb, skb->len, MHI_EOT,
+			       queue);
 	if (unlikely(err)) {
 		net_err_ratelimited("%s: Failed to queue TX buf (%d)\n",
 				    ndev->name, err);
@@ -84,11 +96,12 @@ static netdev_tx_t mhi_ndo_xmit(struct sk_buff *skb, struct net_device *ndev)
 		goto exit_drop;
 	}
 
-	if (mhi_queue_is_full(mdev, DMA_TO_DEVICE))
-		netif_stop_queue(ndev);
+	if (mhi_queue_is_full_chid(mdev, DMA_TO_DEVICE, queue)) {
+		txq = netdev_get_tx_queue(ndev, queue);
+		netif_tx_stop_queue(txq);
+	}
 
 	return NETDEV_TX_OK;
-
 exit_drop:
 	u64_stats_update_begin(&mhi_netdev->stats.tx_syncp);
 	u64_stats_inc(&mhi_netdev->stats.tx_dropped);
@@ -119,11 +132,24 @@ static void mhi_ndo_get_stats64(struct net_device *ndev,
 	} while (u64_stats_fetch_retry_irq(&mhi_netdev->stats.tx_syncp, start));
 }
 
+static u16 mhi_ndo_select_queue(struct net_device *ndev, struct sk_buff *skb,
+				struct net_device *sb_dev)
+{
+	struct mhi_net_dev *mhi_netdev = netdev_priv(ndev);
+	int queue = 0;
+
+	if (mhi_netdev->queue)
+		queue = skb_get_queue_mapping(skb);
+
+	return queue;
+}
+
 static const struct net_device_ops mhi_netdev_ops = {
 	.ndo_open               = mhi_ndo_open,
 	.ndo_stop               = mhi_ndo_stop,
 	.ndo_start_xmit         = mhi_ndo_xmit,
 	.ndo_get_stats64	= mhi_ndo_get_stats64,
+	.ndo_select_queue	= mhi_ndo_select_queue,
 };
 
 static void mhi_net_setup(struct net_device *ndev)
@@ -141,14 +167,16 @@ static void mhi_net_setup(struct net_device *ndev)
 }
 
 static struct sk_buff *mhi_net_skb_agg(struct mhi_net_dev *mhi_netdev,
-				       struct sk_buff *skb)
+				       struct sk_buff *skb, int idx)
 {
-	struct sk_buff *head = mhi_netdev->skbagg_head;
-	struct sk_buff *tail = mhi_netdev->skbagg_tail;
+	struct sk_buff *head;
+	struct sk_buff *tail;
 
+	head = mhi_netdev->skbagg_head[idx];
+	tail = mhi_netdev->skbagg_tail[idx];
 	/* This is non-paged skb chaining using frag_list */
 	if (!head) {
-		mhi_netdev->skbagg_head = skb;
+		mhi_netdev->skbagg_head[idx] = skb;
 		return skb;
 	}
 
@@ -161,9 +189,9 @@ static struct sk_buff *mhi_net_skb_agg(struct mhi_net_dev *mhi_netdev,
 	head->data_len += skb->len;
 	head->truesize += skb->truesize;
 
-	mhi_netdev->skbagg_tail = skb;
+	mhi_netdev->skbagg_tail[idx] = skb;
 
-	return mhi_netdev->skbagg_head;
+	return mhi_netdev->skbagg_head[idx];
 }
 
 static void mhi_net_dl_callback(struct mhi_device *mhi_dev,
@@ -172,8 +200,16 @@ static void mhi_net_dl_callback(struct mhi_device *mhi_dev,
 	struct mhi_net_dev *mhi_netdev = dev_get_drvdata(&mhi_dev->dev);
 	struct sk_buff *skb = mhi_res->buf_addr;
 	int free_desc_count;
+	int queue_idx;
+
+	queue_idx =  mhi_res->chan_idx;
+
+	free_desc_count = mhi_get_free_desc_count_chid(mhi_dev, DMA_FROM_DEVICE, queue_idx);
 
-	free_desc_count = mhi_get_free_desc_count(mhi_dev, DMA_FROM_DEVICE);
+	if (queue_idx > mhi_dev->num_rx_chan)
+		return;
+
+	skb_record_rx_queue(skb, queue_idx);
 
 	if (unlikely(mhi_res->transaction_status)) {
 		switch (mhi_res->transaction_status) {
@@ -187,7 +223,7 @@ static void mhi_net_dl_callback(struct mhi_device *mhi_dev,
 			netdev_warn_once(mhi_netdev->ndev,
 					 "Fragmented packets received, fix MTU?\n");
 			skb_put(skb, mhi_res->bytes_xferd);
-			mhi_net_skb_agg(mhi_netdev, skb);
+			mhi_net_skb_agg(mhi_netdev, skb, queue_idx);
 			break;
 		case -ENOTCONN:
 			/* MHI layer stopping/resetting the DL channel */
@@ -203,10 +239,10 @@ static void mhi_net_dl_callback(struct mhi_device *mhi_dev,
 	} else {
 		skb_put(skb, mhi_res->bytes_xferd);
 
-		if (mhi_netdev->skbagg_head) {
+		if (mhi_netdev->skbagg_head[queue_idx]) {
 			/* Aggregate the final fragment */
-			skb = mhi_net_skb_agg(mhi_netdev, skb);
-			mhi_netdev->skbagg_head = NULL;
+			skb = mhi_net_skb_agg(mhi_netdev, skb, queue_idx);
+			mhi_netdev->skbagg_head[queue_idx] = NULL;
 		}
 
 		switch (skb->data[0] & 0xf0) {
@@ -229,7 +265,7 @@ static void mhi_net_dl_callback(struct mhi_device *mhi_dev,
 	}
 
 	/* Refill if RX buffers queue becomes low */
-	if (free_desc_count >= mhi_netdev->rx_queue_sz / 2)
+	if (free_desc_count >= mhi_netdev->rx_queue_sz[queue_idx] / 2)
 		schedule_delayed_work(&mhi_netdev->rx_refill, 0);
 }
 
@@ -240,6 +276,13 @@ static void mhi_net_ul_callback(struct mhi_device *mhi_dev,
 	struct net_device *ndev = mhi_netdev->ndev;
 	struct mhi_device *mdev = mhi_netdev->mdev;
 	struct sk_buff *skb = mhi_res->buf_addr;
+	struct netdev_queue *txq;
+	int queue_idx;
+
+	queue_idx = mhi_res->chan_idx;
+
+	if (queue_idx > mhi_dev->num_tx_chan)
+		return;
 
 	/* Hardware has consumed the buffer, so free the skb (which is not
 	 * freed by the MHI stack) and perform accounting.
@@ -261,8 +304,12 @@ static void mhi_net_ul_callback(struct mhi_device *mhi_dev,
 	}
 	u64_stats_update_end(&mhi_netdev->stats.tx_syncp);
 
-	if (netif_queue_stopped(ndev) && !mhi_queue_is_full(mdev, DMA_TO_DEVICE))
-		netif_wake_queue(ndev);
+	/* get the right tx_queue*/
+	txq = netdev_get_tx_queue(ndev, queue_idx);
+
+	if (netif_tx_queue_stopped(txq) && !mhi_queue_is_full_chid(mdev,
+			 DMA_TO_DEVICE, queue_idx))
+		netif_tx_wake_queue(txq);
 }
 
 static void mhi_net_rx_refill_work(struct work_struct *work)
@@ -273,47 +320,67 @@ static void mhi_net_rx_refill_work(struct work_struct *work)
 	struct mhi_device *mdev = mhi_netdev->mdev;
 	struct sk_buff *skb;
 	unsigned int size;
-	int err;
+	int err, i;
+	bool sched = false;
 
 	size = mhi_netdev->mru ? mhi_netdev->mru : READ_ONCE(ndev->mtu);
 
-	while (!mhi_queue_is_full(mdev, DMA_FROM_DEVICE)) {
-		skb = netdev_alloc_skb(ndev, size);
-		if (unlikely(!skb))
-			break;
-
-		err = mhi_queue_skb(mdev, DMA_FROM_DEVICE, skb, size, MHI_EOT);
-		if (unlikely(err)) {
-			net_err_ratelimited("%s: Failed to queue RX buf (%d)\n",
-					    ndev->name, err);
-			kfree_skb(skb);
-			break;
+	for (i=0; i<mdev->num_rx_chan; i++) {
+		while (!mhi_queue_is_full_chid(mdev, DMA_FROM_DEVICE, i)) {
+			skb = netdev_alloc_skb(ndev, size);
+			if (unlikely(!skb))
+				break;
+
+			err = mhi_queue_skb_ch(mdev, DMA_FROM_DEVICE, skb, size,
+					       MHI_EOT, i);
+			if (unlikely(err)) {
+				net_err_ratelimited("%s: Failed to queue RX buf (%d)\n",
+						    ndev->name, err);
+				kfree_skb(skb);
+				break;
+			}
+
+			/* Do not hog the CPU if rx buffers are consumed faster than
+			 * queued (unlikely).
+			 */
+			cond_resched();
 		}
-
-		/* Do not hog the CPU if rx buffers are consumed faster than
-		 * queued (unlikely).
-		 */
-		cond_resched();
 	}
 
 	/* If we're still starved of rx buffers, reschedule later */
-	if (mhi_get_free_desc_count(mdev, DMA_FROM_DEVICE) == mhi_netdev->rx_queue_sz)
+	for (i=0; i<mdev->num_rx_chan; i++) {
+		if (mhi_get_free_desc_count_chid(mdev, DMA_FROM_DEVICE, i) == mhi_netdev->rx_queue_sz[i])
+			sched = true;
+	}
+
+	if (sched)
 		schedule_delayed_work(&mhi_netdev->rx_refill, HZ / 2);
 }
 
 static int mhi_net_newlink(struct mhi_device *mhi_dev, struct net_device *ndev)
 {
 	struct mhi_net_dev *mhi_netdev;
-	int err;
+	int err, i;
 
 	mhi_netdev = netdev_priv(ndev);
 
 	dev_set_drvdata(&mhi_dev->dev, mhi_netdev);
 	mhi_netdev->ndev = ndev;
 	mhi_netdev->mdev = mhi_dev;
-	mhi_netdev->skbagg_head = NULL;
+	mhi_netdev->skbagg_head = kmalloc(sizeof(struct sk_buff *) * mhi_dev->num_rx_chan, GFP_KERNEL);
+	if (err)
+		goto out_dev_free;
+	mhi_netdev->skbagg_tail = kmalloc(sizeof(struct sk_buff *) * mhi_dev->num_rx_chan, GFP_KERNEL);
+	if (err)
+		goto out_err_rx_skbagg;
+
 	mhi_netdev->mru = mhi_dev->mhi_cntrl->mru;
 
+	mhi_netdev->queue = mhi_dev->queue;
+	mhi_netdev->rx_queue_sz = kmalloc(sizeof(u32) * mhi_dev->num_rx_chan, GFP_KERNEL);
+	if (err)
+		goto out_err_rx_skbagg_tail;
+
 	INIT_DELAYED_WORK(&mhi_netdev->rx_refill, mhi_net_rx_refill_work);
 	u64_stats_init(&mhi_netdev->stats.rx_syncp);
 	u64_stats_init(&mhi_netdev->stats.tx_syncp);
@@ -321,27 +388,48 @@ static int mhi_net_newlink(struct mhi_device *mhi_dev, struct net_device *ndev)
 	/* Start MHI channels */
 	err = mhi_prepare_for_transfer(mhi_dev);
 	if (err)
-		return err;
+		goto out_err_rx_que;
 
-	/* Number of transfer descriptors determines size of the queue */
-	mhi_netdev->rx_queue_sz = mhi_get_free_desc_count(mhi_dev, DMA_FROM_DEVICE);
+	for (i=0; i < mhi_dev->num_rx_chan; i++) {
+		/* Number of transfer descriptors determines size of the queue */
+		mhi_netdev->rx_queue_sz[i] = mhi_get_free_desc_count_chid(mhi_dev,
+						DMA_FROM_DEVICE, i);
+		mhi_netdev->skbagg_head[i] = NULL;
+	}
 
 	err = register_netdev(ndev);
 	if (err)
-		return err;
+		goto out_err_ready;
 
 	return 0;
+
+out_err_ready:
+	mhi_unprepare_from_transfer(mhi_dev);
+out_err_rx_que:
+	kfree(mhi_netdev->rx_queue_sz);
+out_err_rx_skbagg_tail:
+	kfree(mhi_netdev->skbagg_tail);
+out_err_rx_skbagg:
+	kfree(mhi_netdev->skbagg_head);
+out_dev_free:
+	return err;
 }
 
 static void mhi_net_dellink(struct mhi_device *mhi_dev, struct net_device *ndev)
 {
 	struct mhi_net_dev *mhi_netdev = netdev_priv(ndev);
+	int i;
 
 	unregister_netdev(ndev);
 
 	mhi_unprepare_from_transfer(mhi_dev);
 
-	kfree_skb(mhi_netdev->skbagg_head);
+	for (i=0; i < mhi_dev->num_rx_chan; i++)
+		kfree_skb(mhi_netdev->skbagg_head[i]);
+
+	kfree(mhi_netdev->rx_queue_sz);
+	kfree(mhi_netdev->skbagg_head);
+	kfree(mhi_netdev->skbagg_tail);
 
 	free_netdev(ndev);
 
@@ -355,8 +443,9 @@ static int mhi_net_probe(struct mhi_device *mhi_dev,
 	struct net_device *ndev;
 	int err;
 
-	ndev = alloc_netdev(sizeof(struct mhi_net_dev), info->netname,
-			    NET_NAME_PREDICTABLE, mhi_net_setup);
+	ndev = alloc_netdev_mqs(sizeof(struct mhi_net_dev), info->netname,
+			    NET_NAME_PREDICTABLE, mhi_net_setup,
+			    mhi_dev->num_tx_chan, mhi_dev->num_rx_chan);
 	if (!ndev)
 		return -ENOMEM;
 
-- 
2.45.2

