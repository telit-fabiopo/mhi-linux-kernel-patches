From 45632d908b824f7f97c3a7c19f2bc80507a7cbe4 Mon Sep 17 00:00:00 2001
From: Vivek Pernamitta <quic_vpernami@quicinc.com>
Date: Tue, 23 Apr 2024 16:07:59 +0530
Subject: [PATCH 22/26] bus: mhi: Add MHI support to bind multiple channels in
 to single device

Add support to bind multiple channels to single mhi_device.
so each device can handle multiple channels both tx and rx.

Modifying the current mhi_device structure to map multiple
TX/UL and RX/DL channels (updating to array to mhi_device.ul_chan
and mhi_device.dl_chan channel) instead of single UL and DL
channel.

Providing backward compatible to legacy API's
mhi_queue_skb/dma/buf which will map to channel index
0 by default.
Add support for data transfer/querying/cheding the ring
size for specific channel in mhi_device based on channel
index in device.

Each client driver can queue data using mhi_queue_skb/dma/buf_ch
and completion callback for any traffic send/received over channel
will get receive channel index through mhi_result->chan_idx.
so client driver which support multiple channels binding can
get channel index from mhi_result->chan_idx.

Signed-off-by: Vivek Pernamitta <quic_vpernami@quicinc.com>
Change-Id: I2cb8cb22bee74f1f4691096113365f2a398c0454
---
 drivers/bus/mhi/host/debugfs.c  |  15 +-
 drivers/bus/mhi/host/init.c     | 110 +++++++++---
 drivers/bus/mhi/host/internal.h |   4 +
 drivers/bus/mhi/host/main.c     | 285 +++++++++++++++++++++++---------
 drivers/net/wwan/mhi_wwan_dtr.c |   2 +-
 include/linux/mhi.h             |  85 +++++++++-
 6 files changed, 393 insertions(+), 108 deletions(-)

diff --git a/drivers/bus/mhi/host/debugfs.c b/drivers/bus/mhi/host/debugfs.c
index cfec7811dfbb..c1fb5acd7015 100644
--- a/drivers/bus/mhi/host/debugfs.c
+++ b/drivers/bus/mhi/host/debugfs.c
@@ -130,6 +130,7 @@ static int mhi_debugfs_channels_show(struct seq_file *m, void *d)
 static int mhi_device_info_show(struct device *dev, void *data)
 {
 	struct mhi_device *mhi_dev;
+	int i;
 
 	if (dev->bus != &mhi_bus_type)
 		return 0;
@@ -141,10 +142,16 @@ static int mhi_device_info_show(struct device *dev, void *data)
 		   mhi_dev->dev_wake);
 
 	/* for transfer device types only */
-	if (mhi_dev->dev_type == MHI_DEVICE_XFER)
-		seq_printf((struct seq_file *)data, " channels: %u(UL)/%u(DL)",
-			   mhi_dev->ul_chan_id, mhi_dev->dl_chan_id);
-
+	if (mhi_dev->dev_type == MHI_DEVICE_XFER) {
+		for (i=0; i < mhi_dev->num_tx_chan; i++) {
+			seq_printf((struct seq_file *)data, " channels: %u(UL)",
+				   mhi_dev->ul_chan_id[i]);
+		}
+		for (i=0; i < mhi_dev->num_rx_chan; i++) {
+			seq_printf((struct seq_file *)data, "/%u(DL)",
+				    mhi_dev->dl_chan_id[i]);
+		}
+	}
 	seq_puts((struct seq_file *)data, "\n");
 
 	return 0;
diff --git a/drivers/bus/mhi/host/init.c b/drivers/bus/mhi/host/init.c
index 731543c7f3d8..f99f7a7a215c 100644
--- a/drivers/bus/mhi/host/init.c
+++ b/drivers/bus/mhi/host/init.c
@@ -1226,6 +1226,7 @@ EXPORT_SYMBOL_GPL(mhi_unprepare_after_power_down);
 static void mhi_release_device(struct device *dev)
 {
 	struct mhi_device *mhi_dev = to_mhi_device(dev);
+	int i;
 
 	/*
 	 * We need to set the mhi_chan->mhi_dev to NULL here since the MHI
@@ -1233,11 +1234,17 @@ static void mhi_release_device(struct device *dev)
 	 * associated with it is NULL. This scenario will happen during the
 	 * controller suspend and resume.
 	 */
-	if (mhi_dev->ul_chan)
-		mhi_dev->ul_chan->mhi_dev = NULL;
+	for (i=0; i < mhi_dev->num_tx_chan; i++)
+		mhi_dev->ul_chan[i]->mhi_dev = NULL;
+
+	for (i=0; i < mhi_dev->num_tx_chan; i++)
+		mhi_dev->dl_chan[i]->mhi_dev = NULL;
 
-	if (mhi_dev->dl_chan)
-		mhi_dev->dl_chan->mhi_dev = NULL;
+	kfree(mhi_dev->ul_chan);
+	kfree(mhi_dev->dl_chan);
+
+	kfree(mhi_dev->ul_chan_id);
+	kfree(mhi_dev->dl_chan_id);
 
 	kfree(mhi_dev);
 }
@@ -1277,9 +1284,9 @@ static int mhi_driver_probe(struct device *dev)
 	struct device_driver *drv = dev->driver;
 	struct mhi_driver *mhi_drv = to_mhi_driver(drv);
 	struct mhi_event *mhi_event;
-	struct mhi_chan *ul_chan = mhi_dev->ul_chan;
-	struct mhi_chan *dl_chan = mhi_dev->dl_chan;
-	int ret;
+	struct mhi_chan *ul_chan;
+	struct mhi_chan *dl_chan;
+	int ret, i;
 
 	/* Bring device out of LPM */
 	ret = mhi_device_get_sync(mhi_dev);
@@ -1288,7 +1295,12 @@ static int mhi_driver_probe(struct device *dev)
 
 	ret = -EINVAL;
 
-	if (ul_chan) {
+	for (i=0; i < mhi_dev->num_tx_chan; i++) {
+
+		ul_chan = mhi_dev->ul_chan[i];
+		if (!ul_chan)
+			break;
+
 		/*
 		 * If channel supports LPM notifications then status_cb should
 		 * be provided
@@ -1302,9 +1314,12 @@ static int mhi_driver_probe(struct device *dev)
 
 		ul_chan->xfer_cb = mhi_drv->ul_xfer_cb;
 	}
-
 	ret = -EINVAL;
-	if (dl_chan) {
+	for (i=0; i < mhi_dev->num_rx_chan; i++) {
+
+		dl_chan = mhi_dev->dl_chan[i];
+		if (!dl_chan)
+			break;
 		/*
 		 * If channel supports LPM notifications then status_cb should
 		 * be provided
@@ -1352,19 +1367,21 @@ static int mhi_driver_remove(struct device *dev)
 	struct mhi_driver *mhi_drv = to_mhi_driver(dev->driver);
 	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
 	struct mhi_chan *mhi_chan;
-	enum mhi_ch_state ch_state[] = {
-		MHI_CH_STATE_DISABLED,
-		MHI_CH_STATE_DISABLED
-	};
-	int dir;
+	int *ul_ch_state;
+	int *dl_ch_state;
+	int i;
 
+	
 	/* Skip if it is a controller device */
 	if (mhi_dev->dev_type == MHI_DEVICE_CONTROLLER)
 		return 0;
 
-	/* Reset both channels */
-	for (dir = 0; dir < 2; dir++) {
-		mhi_chan = dir ? mhi_dev->ul_chan : mhi_dev->dl_chan;
+	ul_ch_state = kmalloc(mhi_dev->num_tx_chan * sizeof(int), GFP_KERNEL);
+	dl_ch_state = kmalloc(mhi_dev->num_rx_chan * sizeof(int), GFP_KERNEL);
+	
+	/* remove all Tx and RX channel */
+	for (i=0; i < mhi_dev->num_tx_chan; i++) {
+		mhi_chan = mhi_dev->ul_chan[i];
 
 		if (!mhi_chan)
 			continue;
@@ -1378,7 +1395,33 @@ static int mhi_driver_remove(struct device *dev)
 		/* Set the channel state to disabled */
 		mutex_lock(&mhi_chan->mutex);
 		write_lock_irq(&mhi_chan->lock);
-		ch_state[dir] = mhi_chan->ch_state;
+		ul_ch_state[i] = mhi_chan->ch_state;
+		mhi_chan->ch_state = MHI_CH_STATE_SUSPENDED;
+		write_unlock_irq(&mhi_chan->lock);
+
+		/* Reset the non-offload channel */
+		if (!mhi_chan->offload_ch)
+			mhi_reset_chan(mhi_cntrl, mhi_chan);
+
+		mutex_unlock(&mhi_chan->mutex);
+	}
+
+	for (i=0; i < mhi_dev->num_rx_chan; i++) {
+		mhi_chan = mhi_dev->dl_chan[i];
+
+		if (!mhi_chan)
+			continue;
+
+		/* Wake all threads waiting for completion */
+		write_lock_irq(&mhi_chan->lock);
+		mhi_chan->ccs = MHI_EV_CC_INVALID;
+		complete_all(&mhi_chan->completion);
+		write_unlock_irq(&mhi_chan->lock);
+
+		/* Set the channel state to disabled */
+		mutex_lock(&mhi_chan->mutex);
+		write_lock_irq(&mhi_chan->lock);
+		dl_ch_state[i] = mhi_chan->ch_state;
 		mhi_chan->ch_state = MHI_CH_STATE_SUSPENDED;
 		write_unlock_irq(&mhi_chan->lock);
 
@@ -1391,17 +1434,33 @@ static int mhi_driver_remove(struct device *dev)
 
 	mhi_drv->remove(mhi_dev);
 
-	/* De-init channel if it was enabled */
-	for (dir = 0; dir < 2; dir++) {
-		mhi_chan = dir ? mhi_dev->ul_chan : mhi_dev->dl_chan;
+	/* De-init TX and RX channel if it was enabled */
+	for (i=0; i < mhi_dev->num_tx_chan; i++) {
+		mhi_chan = mhi_dev->ul_chan[i];
+		if (!mhi_chan)
+			continue;
+
+		mutex_lock(&mhi_chan->mutex);
+
+		if ((ul_ch_state[i] == MHI_CH_STATE_ENABLED ||
+		     ul_ch_state[i] == MHI_CH_STATE_STOP) &&
+		    !mhi_chan->offload_ch)
+			mhi_deinit_chan_ctxt(mhi_cntrl, mhi_chan);
+
+		mhi_chan->ch_state = MHI_CH_STATE_DISABLED;
+
+		mutex_unlock(&mhi_chan->mutex);
+	}
 
+	for (i=0; i < mhi_dev->num_rx_chan; i++) {
+		mhi_chan = mhi_dev->dl_chan[i];
 		if (!mhi_chan)
 			continue;
 
 		mutex_lock(&mhi_chan->mutex);
 
-		if ((ch_state[dir] == MHI_CH_STATE_ENABLED ||
-		     ch_state[dir] == MHI_CH_STATE_STOP) &&
+		if ((dl_ch_state[i] == MHI_CH_STATE_ENABLED ||
+		     dl_ch_state[i] == MHI_CH_STATE_STOP) &&
 		    !mhi_chan->offload_ch)
 			mhi_deinit_chan_ctxt(mhi_cntrl, mhi_chan);
 
@@ -1410,6 +1469,9 @@ static int mhi_driver_remove(struct device *dev)
 		mutex_unlock(&mhi_chan->mutex);
 	}
 
+	kfree(ul_ch_state);
+	kfree(dl_ch_state);
+
 	while (mhi_dev->dev_wake)
 		mhi_device_put(mhi_dev);
 
diff --git a/drivers/bus/mhi/host/internal.h b/drivers/bus/mhi/host/internal.h
index 62d1b97c9eef..07412db2ea3d 100644
--- a/drivers/bus/mhi/host/internal.h
+++ b/drivers/bus/mhi/host/internal.h
@@ -242,6 +242,10 @@ struct mhi_chan {
 	bool offload_ch;
 	bool pre_alloc;
 	bool wake_capable;
+	/* If muliple channel needs to be collocated for single client*/
+	bool queue;
+	u32 num_ul_chan;
+	u32 num_dl_chan;
 };
 
 /* Default MHI timeout */
diff --git a/drivers/bus/mhi/host/main.c b/drivers/bus/mhi/host/main.c
index 0082fde93f19..b0d5292224e6 100644
--- a/drivers/bus/mhi/host/main.c
+++ b/drivers/bus/mhi/host/main.c
@@ -278,6 +278,7 @@ int mhi_destroy_device(struct device *dev, void *data)
 	struct mhi_device *mhi_dev;
 	struct mhi_controller *mhi_cntrl;
 	enum mhi_ee_type ee = MHI_EE_MAX;
+	int i;
 
 	if (dev->bus != &mhi_bus_type)
 		return 0;
@@ -289,32 +290,41 @@ int mhi_destroy_device(struct device *dev, void *data)
 	if (mhi_dev->dev_type == MHI_DEVICE_CONTROLLER)
 		return 0;
 
-	ul_chan = mhi_dev->ul_chan;
-	dl_chan = mhi_dev->dl_chan;
+	for (i=0; i < mhi_dev->num_tx_chan; i++) {
+		ul_chan = mhi_dev->ul_chan[i];
 
-	/*
-	 * If execution environment is specified, remove only those devices that
-	 * started in them based on ee_mask for the channels as we move on to a
-	 * different execution environment
-	 */
-	if (data)
-		ee = *(enum mhi_ee_type *)data;
+		/*
+		 * If execution environment is specified, remove only those devices that
+		 * started in them based on ee_mask for the channels as we move on to a
+		 * different execution environment
+		 */
+		if (data)
+			ee = *(enum mhi_ee_type *)data;
 
-	/*
-	 * For the suspend and resume case, this function will get called
-	 * without mhi_unregister_controller(). Hence, we need to drop the
-	 * references to mhi_dev created for ul and dl channels. We can
-	 * be sure that there will be no instances of mhi_dev left after
-	 * this.
-	 */
-	if (ul_chan) {
+		/*
+		 * For the suspend and resume case, this function will get called
+		 * without mhi_unregister_controller(). Hence, we need to drop the
+		 * references to mhi_dev created for ul and dl channels. We can
+		 * be sure that there will be no instances of mhi_dev left after
+		 * this.
+		 */
 		if (ee != MHI_EE_MAX && !(ul_chan->ee_mask & BIT(ee)))
 			return 0;
 
 		put_device(&ul_chan->mhi_dev->dev);
 	}
 
-	if (dl_chan) {
+	for (i=0; i < mhi_dev->num_rx_chan; i++) {
+		dl_chan = mhi_dev->dl_chan[i];
+
+		/*
+		* If execution environment is specified, remove only those devices that
+		* started in them based on ee_mask for the channels as we move on to a
+		* different execution environment
+		*/
+		if (data)
+			ee = *(enum mhi_ee_type *)data;
+
 		if (ee != MHI_EE_MAX && !(dl_chan->ee_mask & BIT(ee)))
 			return 0;
 
@@ -336,16 +346,28 @@ int mhi_destroy_device(struct device *dev, void *data)
 	return 0;
 }
 
-int mhi_get_free_desc_count(struct mhi_device *mhi_dev,
-				enum dma_data_direction dir)
+int mhi_get_free_desc_count_chid(struct mhi_device *mhi_dev,
+				enum dma_data_direction dir, int idx)
 {
 	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	struct mhi_ring *tre_ring;
 	struct mhi_chan *mhi_chan = (dir == DMA_TO_DEVICE) ?
-		mhi_dev->ul_chan : mhi_dev->dl_chan;
-	struct mhi_ring *tre_ring = &mhi_chan->tre_ring;
+		mhi_dev->ul_chan[idx] : mhi_dev->dl_chan[idx];
+
+	if (!mhi_chan)
+		return -ENODEV;
+
+	tre_ring = &mhi_chan->tre_ring;
 
 	return get_nr_avail_ring_elements(mhi_cntrl, tre_ring);
 }
+EXPORT_SYMBOL_GPL(mhi_get_free_desc_count_chid);
+
+int mhi_get_free_desc_count(struct mhi_device *mhi_dev,
+				enum dma_data_direction dir)
+{
+	return mhi_get_free_desc_count_chid(mhi_dev, dir, 0);
+}
 EXPORT_SYMBOL_GPL(mhi_get_free_desc_count);
 
 void mhi_notify(struct mhi_device *mhi_dev, enum mhi_callback cb_reason)
@@ -365,30 +387,49 @@ EXPORT_SYMBOL_GPL(mhi_notify);
 /* Bind MHI channels to MHI devices */
 void mhi_create_devices(struct mhi_controller *mhi_cntrl)
 {
-	struct mhi_chan *mhi_chan;
+	struct mhi_chan *mhi_chan, *m_chan;
 	struct mhi_device *mhi_dev;
 	struct device *dev = &mhi_cntrl->mhi_dev->dev;
-	int i, ret;
+	int i, ret, j;
+	u32 tx_queue_el, rx_queue_el;
+	bool match;
+	u32 tx, rx;
 
 	mhi_chan = mhi_cntrl->mhi_chan;
 	for (i = 0; i < mhi_cntrl->max_chan; i++, mhi_chan++) {
+		tx_queue_el = 0;
+		rx_queue_el = 0;
+		match = false;
+
 		if (!mhi_chan->configured || mhi_chan->mhi_dev ||
 		    !(mhi_chan->ee_mask & BIT(mhi_cntrl->ee)))
 			continue;
+
 		mhi_dev = mhi_alloc_device(mhi_cntrl);
 		if (IS_ERR(mhi_dev))
 			return;
-
 		mhi_dev->dev_type = MHI_DEVICE_XFER;
+
+		mhi_dev->queue = mhi_chan->queue;
+		tx = mhi_chan->num_ul_chan ? mhi_chan->num_ul_chan: 1;
+		rx = mhi_chan->num_dl_chan ? mhi_chan->num_dl_chan: 1;
+		mhi_dev->ul_chan = kmalloc((sizeof(struct mhi_chan*) * tx), GFP_KERNEL);
+		mhi_dev->dl_chan = kmalloc((sizeof(struct mhi_chan*) * rx), GFP_KERNEL);
+
+		mhi_dev->ul_chan_id = kmalloc(sizeof(int) * tx, GFP_KERNEL);
+		mhi_dev->dl_chan_id = kmalloc(sizeof(int) * rx, GFP_KERNEL);
+
 		switch (mhi_chan->dir) {
 		case DMA_TO_DEVICE:
-			mhi_dev->ul_chan = mhi_chan;
-			mhi_dev->ul_chan_id = mhi_chan->chan;
+			mhi_dev->ul_chan[0] = mhi_chan;
+			mhi_dev->ul_chan_id[0] = mhi_chan->chan;
+			tx_queue_el++;
 			break;
 		case DMA_FROM_DEVICE:
 			/* We use dl_chan as offload channels */
-			mhi_dev->dl_chan = mhi_chan;
-			mhi_dev->dl_chan_id = mhi_chan->chan;
+			mhi_dev->dl_chan[0] = mhi_chan;
+			mhi_dev->dl_chan_id[0] = mhi_chan->chan;
+			rx_queue_el++;
 			break;
 		default:
 			dev_err(dev, "Direction not supported\n");
@@ -399,23 +440,31 @@ void mhi_create_devices(struct mhi_controller *mhi_cntrl)
 		get_device(&mhi_dev->dev);
 		mhi_chan->mhi_dev = mhi_dev;
 
-		/* Check next channel if it matches */
-		if ((i + 1) < mhi_cntrl->max_chan && mhi_chan[1].configured) {
-			if (!strcmp(mhi_chan[1].name, mhi_chan->name)) {
-				i++;
-				mhi_chan++;
-				if (mhi_chan->dir == DMA_TO_DEVICE) {
-					mhi_dev->ul_chan = mhi_chan;
-					mhi_dev->ul_chan_id = mhi_chan->chan;
+		/* Parse the entire list of channel to see if the channel needs to be collicated */
+		m_chan = mhi_chan;
+		m_chan++;
+
+		for (j=i+1; j < mhi_cntrl->max_chan; j++, m_chan++) {
+			if (!m_chan->configured)
+				continue;
+			if (!strcmp(m_chan->name, mhi_chan->name)) {
+				if (m_chan->dir == DMA_TO_DEVICE) {
+					mhi_dev->ul_chan[tx_queue_el] = m_chan;
+					mhi_dev->ul_chan_id[tx_queue_el] = m_chan->chan;
+					tx_queue_el++;
 				} else {
-					mhi_dev->dl_chan = mhi_chan;
-					mhi_dev->dl_chan_id = mhi_chan->chan;
+					mhi_dev->dl_chan[rx_queue_el] = m_chan;
+					mhi_dev->dl_chan_id[rx_queue_el] = m_chan->chan;
+					rx_queue_el++;
 				}
 				get_device(&mhi_dev->dev);
-				mhi_chan->mhi_dev = mhi_dev;
+				m_chan->mhi_dev = mhi_dev;
 			}
 		}
 
+		mhi_dev->num_tx_chan = tx_queue_el;
+		mhi_dev->num_rx_chan = rx_queue_el;
+
 		/* Channel name is same for both UL and DL */
 		mhi_dev->name = mhi_chan->name;
 		dev_set_name(&mhi_dev->dev, "%s_%s",
@@ -423,7 +472,7 @@ void mhi_create_devices(struct mhi_controller *mhi_cntrl)
 			     mhi_dev->name);
 
 		/* Init wakeup source if available */
-		if (mhi_dev->dl_chan && mhi_dev->dl_chan->wake_capable)
+		if (mhi_dev->dl_chan[0] && mhi_dev->dl_chan[0]->wake_capable)
 			device_init_wakeup(&mhi_dev->dev, true);
 
 		ret = device_add(&mhi_dev->dev);
@@ -576,6 +625,27 @@ static void mhi_recycle_ev_ring_element(struct mhi_controller *mhi_cntrl,
 	smp_wmb();
 }
 
+/* returns the channel idx for the mhi dev */
+static int mhi_dev_map_ch_queue_idx(struct mhi_controller *mhi_cntrl,
+				struct mhi_chan *mhi_chan)
+{
+	struct mhi_chan **tmp;
+	struct mhi_device *mhi_dev = mhi_chan->mhi_dev;
+	int i, count;
+
+	count = (mhi_chan->dir == DMA_TO_DEVICE) ? mhi_dev->num_tx_chan :
+						   mhi_dev->num_rx_chan;
+
+	tmp = (mhi_chan->dir == DMA_TO_DEVICE) ? mhi_dev->ul_chan :
+	mhi_dev->dl_chan;
+	for (i=0; i < count; i++) {
+		if (mhi_chan->chan == tmp[i]->chan)
+			return i;
+
+	}
+	return -ENODEV;
+}
+
 static int parse_xfer_event(struct mhi_controller *mhi_cntrl,
 			    struct mhi_ring_element *event,
 			    struct mhi_chan *mhi_chan)
@@ -632,6 +702,8 @@ static int parse_xfer_event(struct mhi_controller *mhi_cntrl,
 
 		result.dir = mhi_chan->dir;
 
+		result.chan_idx = mhi_dev_map_ch_queue_idx(mhi_cntrl, mhi_chan);
+
 		local_rp = tre_ring->rp;
 		while (local_rp != dev_rp) {
 			buf_info = buf_ring->rp;
@@ -671,10 +743,11 @@ static int parse_xfer_event(struct mhi_controller *mhi_cntrl,
 			 * from dropping the packet
 			 */
 			if (mhi_chan->pre_alloc) {
-				if (mhi_queue_buf(mhi_chan->mhi_dev,
+				if (mhi_queue_buf_ch(mhi_chan->mhi_dev,
 						  mhi_chan->dir,
 						  buf_info->cb_buf,
-						  buf_info->len, MHI_EOT)) {
+						  buf_info->len, MHI_EOT,
+						  result.chan_idx)) {
 					dev_err(dev,
 						"Error recycling buffer for chan:%d\n",
 						mhi_chan->chan);
@@ -748,6 +821,8 @@ static int parse_rsc_event(struct mhi_controller *mhi_cntrl,
 
 	read_lock_bh(&mhi_chan->lock);
 
+	result.chan_idx = mhi_dev_map_ch_queue_idx(mhi_cntrl, mhi_chan);
+
 	if (mhi_chan->ch_state != MHI_CH_STATE_ENABLED)
 		goto end_process_rsc_event;
 
@@ -1123,11 +1198,11 @@ static bool mhi_is_ring_full(struct mhi_controller *mhi_cntrl,
 }
 
 static int mhi_queue(struct mhi_device *mhi_dev, struct mhi_buf_info *buf_info,
-		     enum dma_data_direction dir, enum mhi_flags mflags)
+		     enum dma_data_direction dir, enum mhi_flags mflags, int ch_id)
 {
 	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
-	struct mhi_chan *mhi_chan = (dir == DMA_TO_DEVICE) ? mhi_dev->ul_chan :
-							     mhi_dev->dl_chan;
+	struct mhi_chan *mhi_chan = (dir == DMA_TO_DEVICE) ? mhi_dev->ul_chan[ch_id] :
+							     mhi_dev->dl_chan[ch_id];
 	struct mhi_ring *tre_ring = &mhi_chan->tre_ring;
 	unsigned long flags;
 	int ret;
@@ -1168,11 +1243,11 @@ static int mhi_queue(struct mhi_device *mhi_dev, struct mhi_buf_info *buf_info,
 	return ret;
 }
 
-int mhi_queue_skb(struct mhi_device *mhi_dev, enum dma_data_direction dir,
-		  struct sk_buff *skb, size_t len, enum mhi_flags mflags)
+int mhi_queue_skb_ch(struct mhi_device *mhi_dev, enum dma_data_direction dir,
+		     struct sk_buff *skb, size_t len, enum mhi_flags mflags, int ch_id)
 {
-	struct mhi_chan *mhi_chan = (dir == DMA_TO_DEVICE) ? mhi_dev->ul_chan :
-							     mhi_dev->dl_chan;
+	struct mhi_chan *mhi_chan = (dir == DMA_TO_DEVICE) ? mhi_dev->ul_chan[ch_id] :
+							     mhi_dev->dl_chan[ch_id];
 	struct mhi_buf_info buf_info = { };
 
 	buf_info.v_addr = skb->data;
@@ -1182,15 +1257,24 @@ int mhi_queue_skb(struct mhi_device *mhi_dev, enum dma_data_direction dir,
 	if (unlikely(mhi_chan->pre_alloc))
 		return -EINVAL;
 
-	return mhi_queue(mhi_dev, &buf_info, dir, mflags);
+	return mhi_queue(mhi_dev, &buf_info, dir, mflags, ch_id);
+
+}
+EXPORT_SYMBOL_GPL(mhi_queue_skb_ch);
+
+int mhi_queue_skb(struct mhi_device *mhi_dev, enum dma_data_direction dir,
+		  struct sk_buff *skb, size_t len, enum mhi_flags mflags)
+{
+
+	return mhi_queue_skb_ch(mhi_dev, dir, skb, len, mflags, 0);
 }
 EXPORT_SYMBOL_GPL(mhi_queue_skb);
 
-int mhi_queue_dma(struct mhi_device *mhi_dev, enum dma_data_direction dir,
-		  struct mhi_buf *mhi_buf, size_t len, enum mhi_flags mflags)
+int mhi_queue_dma_ch(struct mhi_device *mhi_dev, enum dma_data_direction dir,
+		     struct mhi_buf *mhi_buf, size_t len, enum mhi_flags mflags, int ch_id)
 {
-	struct mhi_chan *mhi_chan = (dir == DMA_TO_DEVICE) ? mhi_dev->ul_chan :
-							     mhi_dev->dl_chan;
+	struct mhi_chan *mhi_chan = (dir == DMA_TO_DEVICE) ? mhi_dev->ul_chan[ch_id] :
+							     mhi_dev->dl_chan[ch_id];
 	struct mhi_buf_info buf_info = { };
 
 	buf_info.p_addr = mhi_buf->dma_addr;
@@ -1201,7 +1285,14 @@ int mhi_queue_dma(struct mhi_device *mhi_dev, enum dma_data_direction dir,
 	if (unlikely(mhi_chan->pre_alloc))
 		return -EINVAL;
 
-	return mhi_queue(mhi_dev, &buf_info, dir, mflags);
+	return mhi_queue(mhi_dev, &buf_info, dir, mflags, ch_id);
+}
+EXPORT_SYMBOL_GPL(mhi_queue_dma_ch);
+
+int mhi_queue_dma(struct mhi_device *mhi_dev, enum dma_data_direction dir,
+		  struct mhi_buf *mhi_buf, size_t len, enum mhi_flags mflags)
+{
+	return mhi_queue_dma_ch(mhi_dev, dir, mhi_buf, len, mflags, 0);
 }
 EXPORT_SYMBOL_GPL(mhi_queue_dma);
 
@@ -1259,8 +1350,8 @@ int mhi_gen_tre(struct mhi_controller *mhi_cntrl, struct mhi_chan *mhi_chan,
 	return 0;
 }
 
-int mhi_queue_buf(struct mhi_device *mhi_dev, enum dma_data_direction dir,
-		  void *buf, size_t len, enum mhi_flags mflags)
+int mhi_queue_buf_ch(struct mhi_device *mhi_dev, enum dma_data_direction dir,
+		  void *buf, size_t len, enum mhi_flags mflags, int ch_id)
 {
 	struct mhi_buf_info buf_info = { };
 
@@ -1268,19 +1359,32 @@ int mhi_queue_buf(struct mhi_device *mhi_dev, enum dma_data_direction dir,
 	buf_info.cb_buf = buf;
 	buf_info.len = len;
 
-	return mhi_queue(mhi_dev, &buf_info, dir, mflags);
+	return mhi_queue(mhi_dev, &buf_info, dir, mflags, ch_id);
+}
+EXPORT_SYMBOL_GPL(mhi_queue_buf_ch);
+
+int mhi_queue_buf(struct mhi_device *mhi_dev, enum dma_data_direction dir,
+		  void *buf, size_t len, enum mhi_flags mflags)
+{
+	return mhi_queue_buf_ch(mhi_dev, dir, buf, len, mflags, 0);
 }
 EXPORT_SYMBOL_GPL(mhi_queue_buf);
 
-bool mhi_queue_is_full(struct mhi_device *mhi_dev, enum dma_data_direction dir)
+bool mhi_queue_is_full_chid(struct mhi_device *mhi_dev, enum dma_data_direction dir, int ch_id)
 {
 	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
 	struct mhi_chan *mhi_chan = (dir == DMA_TO_DEVICE) ?
-					mhi_dev->ul_chan : mhi_dev->dl_chan;
-	struct mhi_ring *tre_ring = &mhi_chan->tre_ring;
+					 mhi_dev->ul_chan[ch_id] : mhi_dev->dl_chan[ch_id];
 
+	struct mhi_ring *tre_ring = &mhi_chan->tre_ring;
 	return mhi_is_ring_full(mhi_cntrl, tre_ring);
 }
+EXPORT_SYMBOL_GPL(mhi_queue_is_full_chid);
+
+bool mhi_queue_is_full(struct mhi_device *mhi_dev, enum dma_data_direction dir)
+{
+	return mhi_queue_is_full_chid(mhi_dev, dir, 0);
+}
 EXPORT_SYMBOL_GPL(mhi_queue_is_full);
 
 int mhi_send_cmd(struct mhi_controller *mhi_cntrl,
@@ -1597,6 +1701,7 @@ static void mhi_reset_data_chan(struct mhi_controller *mhi_cntrl,
 	tre_ring = &mhi_chan->tre_ring;
 	result.transaction_status = -ENOTCONN;
 	result.bytes_xferd = 0;
+	result.chan_idx =  mhi_dev_map_ch_queue_idx(mhi_cntrl, mhi_chan);
 	while (tre_ring->rp != tre_ring->wp) {
 		struct mhi_buf_info *buf_info = buf_ring->rp;
 
@@ -1644,31 +1749,47 @@ void mhi_reset_chan(struct mhi_controller *mhi_cntrl, struct mhi_chan *mhi_chan)
 
 static int __mhi_prepare_for_transfer(struct mhi_device *mhi_dev, unsigned int flags)
 {
-	int ret, dir;
+	int ret, i, j;
 	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
 	struct mhi_chan *mhi_chan;
 
-	for (dir = 0; dir < 2; dir++) {
-		mhi_chan = dir ? mhi_dev->dl_chan : mhi_dev->ul_chan;
+	for (i = 0; i < mhi_dev->num_tx_chan; i++) {
+		mhi_chan =  mhi_dev->ul_chan[i] ;
 		if (!mhi_chan)
 			continue;
 
 		ret = mhi_prepare_channel(mhi_cntrl, mhi_chan, flags);
 		if (ret)
-			goto error_open_chan;
+			goto error_open_tx_chan;
+	}
+
+	for (j = 0; j < mhi_dev->num_rx_chan; j++) {
+		mhi_chan =  mhi_dev->dl_chan[j] ;
+		if (!mhi_chan)
+			continue;
+
+		ret = mhi_prepare_channel(mhi_cntrl, mhi_chan, flags);
+		if (ret)
+			goto error_open_rx_chan;
 	}
 
 	return 0;
 
-error_open_chan:
-	for (--dir; dir >= 0; dir--) {
-		mhi_chan = dir ? mhi_dev->dl_chan : mhi_dev->ul_chan;
+error_open_rx_chan:
+	for (--i; i >= 0; i--) {
+		mhi_chan =  mhi_dev->dl_chan[i] ;
 		if (!mhi_chan)
 			continue;
-
 		mhi_unprepare_channel(mhi_cntrl, mhi_chan);
 	}
 
+error_open_tx_chan:
+	for (--j; j >= 0; j--) {
+		mhi_chan =  mhi_dev->ul_chan[j] ;
+		if (!mhi_chan)
+			continue;
+		mhi_unprepare_channel(mhi_cntrl, mhi_chan);
+	}
 	return ret;
 }
 
@@ -1688,10 +1809,18 @@ void mhi_unprepare_from_transfer(struct mhi_device *mhi_dev)
 {
 	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
 	struct mhi_chan *mhi_chan;
-	int dir;
+	int i;
 
-	for (dir = 0; dir < 2; dir++) {
-		mhi_chan = dir ? mhi_dev->ul_chan : mhi_dev->dl_chan;
+	for (i = 0; i < mhi_dev->num_tx_chan; i++) {
+		mhi_chan = mhi_dev->ul_chan[i];
+		if (!mhi_chan)
+			continue;
+
+		mhi_unprepare_channel(mhi_cntrl, mhi_chan);
+	}
+
+	for (i = 0; i < mhi_dev->num_rx_chan; i++) {
+		mhi_chan = mhi_dev->dl_chan[i];
 		if (!mhi_chan)
 			continue;
 
@@ -1700,10 +1829,10 @@ void mhi_unprepare_from_transfer(struct mhi_device *mhi_dev)
 }
 EXPORT_SYMBOL_GPL(mhi_unprepare_from_transfer);
 
-int mhi_poll(struct mhi_device *mhi_dev, u32 budget)
+int mhi_poll_chid(struct mhi_device *mhi_dev, u32 budget, int ch_id)
 {
 	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
-	struct mhi_chan *mhi_chan = mhi_dev->dl_chan;
+	struct mhi_chan *mhi_chan = mhi_dev->dl_chan[ch_id];
 	struct mhi_event *mhi_event = &mhi_cntrl->mhi_event[mhi_chan->er_index];
 	int ret;
 
@@ -1713,4 +1842,10 @@ int mhi_poll(struct mhi_device *mhi_dev, u32 budget)
 
 	return ret;
 }
+EXPORT_SYMBOL_GPL(mhi_poll_chid);
+
+int mhi_poll(struct mhi_device *mhi_dev, u32 budget)
+{
+	return mhi_poll_chid(mhi_dev, budget, 0);
+}
 EXPORT_SYMBOL_GPL(mhi_poll);
diff --git a/drivers/net/wwan/mhi_wwan_dtr.c b/drivers/net/wwan/mhi_wwan_dtr.c
index 83d571d0feb5..a5efcdfb8247 100644
--- a/drivers/net/wwan/mhi_wwan_dtr.c
+++ b/drivers/net/wwan/mhi_wwan_dtr.c
@@ -32,7 +32,7 @@ int mhi_wwan_dtr_set(struct wwan_port *port, int dtr, int rts)
 
 		dtr_msg.preamble = 0x4C525443;
 		dtr_msg.msg_id = 0x10;
-		dtr_msg.dest_id = mhiwwan->mhi_dev->ul_chan_id;
+		dtr_msg.dest_id = *mhiwwan->mhi_dev->ul_chan_id;
 		dtr_msg.size = sizeof(u32);
 		if (dtr)
 			dtr_msg.msg |= BIT(0);
diff --git a/include/linux/mhi.h b/include/linux/mhi.h
index c275862660cf..50d38c8cb206 100644
--- a/include/linux/mhi.h
+++ b/include/linux/mhi.h
@@ -485,18 +485,24 @@ struct mhi_controller {
  * @dev_type: MHI device type
  * @ul_chan_id: MHI channel id for UL transfer
  * @dl_chan_id: MHI channel id for DL transfer
+ * @num_tx_chan: number of tx channels
+ * @num_rx_chan: number of rx channels
+ * @queue: if mhi device supports muliple pair of TX/RX channels
  * @dev_wake: Device wakeup counter
  */
 struct mhi_device {
 	const struct mhi_device_id *id;
 	const char *name;
 	struct mhi_controller *mhi_cntrl;
-	struct mhi_chan *ul_chan;
-	struct mhi_chan *dl_chan;
+	struct mhi_chan **ul_chan;
+	struct mhi_chan **dl_chan;
 	struct device dev;
 	enum mhi_device_type dev_type;
-	int ul_chan_id;
-	int dl_chan_id;
+	int *ul_chan_id;
+	int *dl_chan_id;
+	bool queue;
+	u32 num_tx_chan;
+	u32 num_rx_chan;
 	u32 dev_wake;
 };
 
@@ -506,12 +512,14 @@ struct mhi_device {
  * @bytes_xferd: # of bytes transferred
  * @dir: Channel direction
  * @transaction_status: Status of last transaction
+ * @chan_idx: compemtion event on ch idx for multi queue
  */
 struct mhi_result {
 	void *buf_addr;
 	size_t bytes_xferd;
 	enum dma_data_direction dir;
 	int transaction_status;
+	int chan_idx;
 };
 
 /**
@@ -634,6 +642,17 @@ void mhi_notify(struct mhi_device *mhi_dev, enum mhi_callback cb_reason);
 int mhi_get_free_desc_count(struct mhi_device *mhi_dev,
 				enum dma_data_direction dir);
 
+/**
+ * mhi_get_free_desc_count - Get transfer ring length
+ * Get # of TD available to queue buffers
+ * @mhi_dev: Device associated with the channels
+ * @dir: Direction of the channel
+ * @idx: chan index in mhi device
+ */
+int mhi_get_free_desc_count_chid(struct mhi_device *mhi_dev,
+				enum dma_data_direction dir, int idx);
+
+
 /**
  * mhi_prepare_for_power_up - Do pre-initialization before power up.
  *                            This is optional, call this before power up if
@@ -791,6 +810,15 @@ void mhi_unprepare_from_transfer(struct mhi_device *mhi_dev);
  */
 int mhi_poll(struct mhi_device *mhi_dev, u32 budget);
 
+/**
+ * mhi_poll - Poll for any available data in DL direction
+ * @mhi_dev: Device associated with the channels
+ * @budget: # of events to process
+ * @ch_id: # MHI channel index in mhi device
+ */
+int mhi_poll_chid(struct mhi_device *mhi_dev, u32 budget, int ch_id);
+
+
 /**
  * mhi_queue_dma - Send or receive DMA mapped buffers from client device
  *                 over MHI channel
@@ -802,7 +830,20 @@ int mhi_poll(struct mhi_device *mhi_dev, u32 budget);
  */
 int mhi_queue_dma(struct mhi_device *mhi_dev, enum dma_data_direction dir,
 		  struct mhi_buf *mhi_buf, size_t len, enum mhi_flags mflags);
+/**
+ * mhi_queue_dma - Send or receive DMA mapped buffers from client device
+ *                 over MHI channel
+ * @mhi_dev: Device associated with the channels
+ * @dir: DMA direction for the channel
+ * @mhi_buf: Buffer for holding the DMA mapped data
+ * @len: Buffer length
+ * @mflags: MHI transfer flags used for the transfer
+ * @ch_id: MHI channel index in mhi device
+ */
 
+int mhi_queue_dma_ch(struct mhi_device *mhi_dev, enum dma_data_direction dir,
+		  struct mhi_buf *mhi_buf, size_t len, enum mhi_flags mflags,
+		   int ch_id);
 /**
  * mhi_queue_buf - Send or receive raw buffers from client device over MHI
  *                 channel
@@ -815,6 +856,19 @@ int mhi_queue_dma(struct mhi_device *mhi_dev, enum dma_data_direction dir,
 int mhi_queue_buf(struct mhi_device *mhi_dev, enum dma_data_direction dir,
 		  void *buf, size_t len, enum mhi_flags mflags);
 
+/**
+ * mhi_queue_buf - Send or receive raw buffers from client device over MHI
+ *                 channel
+ * @mhi_dev: Device associated with the channels
+ * @dir: DMA direction for the channel
+ * @buf: Buffer for holding the data
+ * @len: Buffer length
+ * @mflags: MHI transfer flags used for the transfer
+ * @ch_id: MHI channel index in mhi device
+ */
+int mhi_queue_buf_ch(struct mhi_device *mhi_dev, enum dma_data_direction dir,
+		  void *buf, size_t len, enum mhi_flags mflags, int ch_id);
+
 /**
  * mhi_queue_skb - Send or receive SKBs from client device over MHI channel
  * @mhi_dev: Device associated with the channels
@@ -826,6 +880,20 @@ int mhi_queue_buf(struct mhi_device *mhi_dev, enum dma_data_direction dir,
 int mhi_queue_skb(struct mhi_device *mhi_dev, enum dma_data_direction dir,
 		  struct sk_buff *skb, size_t len, enum mhi_flags mflags);
 
+/**
+ * mhi_queue_skb - Send or receive SKBs from client device over MHI channel
+ * @mhi_dev: Device associated with the channels
+ * @dir: DMA direction for the channel
+ * @skb: Buffer for holding SKBs
+ * @len: Buffer length
+ * @mflags: MHI transfer flags used for the transfer
+ * @ch_id: MHI channel index in mhi device
+ */
+int mhi_queue_skb_ch(struct mhi_device *mhi_dev, enum dma_data_direction dir,
+		  struct sk_buff *skb, size_t len, enum mhi_flags mflags,
+		  int ch_id);
+
+
 /**
  * mhi_queue_is_full - Determine whether queueing new elements is possible
  * @mhi_dev: Device associated with the channels
@@ -833,4 +901,13 @@ int mhi_queue_skb(struct mhi_device *mhi_dev, enum dma_data_direction dir,
  */
 bool mhi_queue_is_full(struct mhi_device *mhi_dev, enum dma_data_direction dir);
 
+/**
+ * mhi_queue_is_full - Determine whether queueing new elements is possible
+ * @mhi_dev: Device associated with the channels
+ * @dir: DMA direction for the channel
+ * @ch_id: mhi channel index in mhi device
+ */
+bool mhi_queue_is_full_chid(struct mhi_device *mhi_dev, enum dma_data_direction dir,
+			    int ch_id);
+
 #endif /* _MHI_H_ */
-- 
2.45.2

